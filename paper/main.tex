\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage[round]{natbib}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\sep}{sep}
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{red},
  stringstyle=\color{red},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\fig}[1]{\includegraphics[width=0.48\textwidth]{#1}}
\newcommand{\figb}[1]{\includegraphics[width=0.95\textwidth]{#1}}
\newcommand{\figz}[1]{\includegraphics[width=0.23\textwidth]{#1.png}}
\newcommand{\m}[1]{\mathbf{#1}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\TT}{\mathcal{T}}

\newcommand{\nhatch}[1]{{\leavevmode\color{blue} Nathan: #1}}

\usepackage{fancyhdr}
\pagestyle{fancy}

\title{Curriculum and Transfer Learning of Hierarchical Locomotion Policies}
\author{TBD}

\begin{document}

\maketitle

Definitions:
\begin{itemize}
  \item HLC: high-level controller
  \item LLC: low-level contoller
  \item HRL: hierarchical reinforcement learning
\end{itemize}

\section{Introduction}

Locomotion is hard because the changing contacts make the dynamics difficult to model.
Recent deep reinforcement learning approaches try to learn this model implicitly via the policy network, which has trouble learning such complicated models, possibly due to the limitations of current optimization methods (like policy gradient).
Such approaches fail in environments that require predictive planning at the level of individual contacts, such as running across irregular terrain, where small miscalculations have disproportionally large effects.

We aim to simplify the learning problem by imposing prior structure on the policy: we split it into a two-level hierarchy.
The higher level breaks the plan into smaller chunks, which are easier to achieve.
The lower level must then execute these smaller chunks.
Each chunk is a single footstep.
This choice simplifies the dynamics that the lower level needs to model, because each step involves no (or very few) changes of contact.
It also simplifies the upper level learning problem, since the controller does not need to handle details of torque, other than knowing which step plans can actually be achieved.

\section{Experiments}

Our experiments are designed to test two hypotheses.

Our primary hypothesis is that this hierarchical architecture can navigate a variety of complicated locomotion environments, learning faster and achieving better results than other reinforcement learning methods.
These locomotion environments include flat ground, inclines, stairs, random inclines, stepping stones, hurdles, and combinations of the above.

Our two metrics for ``learning faster'' are (1) number of simulation frames of experience collected, and (2) total computation time.
We consider both necessary, because the relative expense of more experience versus more computation may vary across application domains.

We collect both quantitative and qualitative metrics to measure ``achieving better results''.
Quantitatively, we compare the average rate of forward progress.
We also consider the robustness of the model to force perturbations and various measures of the ``difficulty'' of the terrain (such as stair height).
Qualitatively, we document motion quality by means of supplementary videos, which we encourage the reader to consult.

Our secondary hypothesis is that the learned policy can be run in real-time while still achieving results comparable to more expensive, model-based optimization methods.

This hypothesis is difficult to test, because model-based optimization methods are difficult to implement for direct comparison.
Thus, our metric is simply the percent real time necessary to compute the control for the next time step, which we compare to numbers reported in optimization-based papers.

Additionally, we conduct ablative analysis of the importance of various components of our approach.
\nhatch{In particular, we want to consider various different HLC / LLC interfaces that we could have used.
For example: should we include a target foot height that the LLC should reach?
(I think this would help when going over hurdles.)
For another example (assuming we use some kind of PD control at the lowest level): how much does performance degrade if we don't use PD control?
(That is, if the LLC outputs raw torques.)}

\section{Policy architecture}

\nhatch{Ideally, maybe we should design this so that walking and running can both be achieved in the same episode.}

\nhatch{Should there be some ensemble of low-level policy networks that the HLC can choose between?
For example, we could take the policy with the highest estimated value given the target step location.
Having some discrete choices like this might be easier than using solely continuous parameters.}

\textbf{High-level observations.}
We assume that perception is solved.
The high-level controller receives proprioceptive information and environmental information.
The environmental information consists of a list of potential surfaces to step on, specified in terms of relative displacement from the agent and surface normal.
\nhatch{Maybe, for polygonal surfaces, we could also report the surface ``size'' (e.g. largest-radius circle that fits inside the polygon).}

\textbf{High-level actions.}
High-level actions are chosen on each heel-down event (once per footstep).
At these events, the HLC outputs a target for the next footstep that it expects the LLC can achieve.
This target consists of several parameters: \nhatch{TODO think about this some more}
\nhatch{It might be possible to incorporate some optimization / search / planning into the HLC, because its actions happen infrequently.
But this would require some objective to be optimized, which might involve implementing a model like a spring-loaded inverted pendulum (SLIP), or inventing some heuristic objective that might characterize good plans.
We might want to implement a model like SLIP anyway, in order to use it as an expert to train a neural network for the HLC (similar to AlphaGo Zero and ExIt).
In any case, only the first step of the plan would be revealed to the LLC.}

\textbf{Low-level observations.}
The low-level controller receives proprioceptive information and the target location for the next footstep.
Unlike the HLC, it does not observe the external environment.

\textbf{Low-level actions.}
On the one hand, we could have low-level actions being the raw torques.
On the other hand, this might make the learning problem harder, so we could specify some more abstract actions and use PD controllers or other analytical formulas to derive the actual torques.
For instance, the output could be a target pose or end-effector position which is chosen more infrequently than the simulation framerate.
Then we could solve an inverse kinematics or inverse dynamics problem to calculate a target pose, which is then achieved using PD controllers.
\nhatch{We probably don't need all of this machinery.
For the initial proof of concept, it might make sense to see how we can do using just raw torques.}
\nhatch{TODO think about this some more.}

\textbf{PD control.}
\nhatch{We might have to rethink this abstraction layer once the ground includes inclines.
For instance, the PD controller might now have to handle different target ankle angles.}

\section{Timeline of work}

The goal is to submit this to ICRA on September 15.

\begin{itemize}
  \item Early June (2 weeks): Proof-of-concept: train a 2D walker low-level controller to follow an irregular step plan on flat ground, including some low-level PD control if necessary.
  \item Late June (2 weeks): Train low-level controller on stepping stones.
    Train high-level controller on flat ground, then on stepping stones.
    Compare performance on stepping stones to baseline deep RL approach (TRPO).
    If we can't get this to work (better than TRPO), we should consider abandoning the project.
  \item July (4 weeks): Implement and train on inclines, stairs, and hurdles.
    Perform ablative analyses.
  \item Early August (2 weeks): I (Nathan) will be on vacation.
  \item Late August and early September (4 weeks): Last-minute experiments.
    Write and revise the paper.
  \item September 15: ICRA deadline.
\end{itemize}

\section{Literature review}

\subsection*{Locomotion on varied terrain}

\begin{itemize}
  \item \cite{peng2016terrain}: Perhaps the most closely related prior work, this paper trains planar characters to run across varied terrain.
    In the discussion section of this paper, they mention three specific directions for extension that essentially exactly define our current project: move to three dimensions, consider more difficult terrains (smaller, more frequent obstacles) that require footstep planning, and use curriculum learning.

  \item \cite{heess2017emergence} trains policies that can, to some extent, handle bumpy ground (see \href{https://www.youtube.com/watch?v=hx_bgoTF7bs&feature=youtu.be&t=1m28s}{this video}).
    However, the agents here do not actually observe the bumps.
    They assume flat ground and depend on policy robustness to handle unexpected terrain variation.
    They do not report quantitative results.
    This approach is likely limited in the kinds of bumps that it can handle.
    In the video, the humanoid trips.

  \item \cite{peng2017deeploco}
    learns locomotion in an environment that includes bumpy ground, but the path they follow is perfectly flat.
    They also use motion capture to train the LLC, and the steps are all of uniform length (0.5 seconds), which we hope to avoid.

  \item \cite{peng2018deepmimic} includes some tasks that involve running across varied terrain.
    They also use a form of curriculum learning, where the policies are first trained on flat ground (and without a vision component), before training then continues on the full problem.
    Their policies are somewhat hierarchical, in the sense that they output a target pose, which is then used as input for a PD controller to generate torques.
    Qualitatively, the policies on these tasks seem to mostly ignore the obstacles, relying on being able to recover from the perturbations generated by collisions.
    In contrast, we would like to avoid the kind of stumbling seen in these videos by introducing another level of hierarchy that generates an explicit footstep plan.


  \item \cite{peng2017learning} uses bumpy ground to test robustness of learned policies, but again the bumps are not actually perceived and cannot be planned around.
    The bump height thus must remain small relative to the agent.
    \nhatch{We should compare this quantitatively, later.} Also, this is 2D.
  \item \cite{manchester2011stable} This paper stands out in that they derive a provably stable controller that they test on a physical robot.
    \nhatch{I haven't read it in detail yet, though.}
\end{itemize}

\subsection*{Hierarchical reinforcement learning}

\begin{itemize}
  \item \cite{heess2016learning} discusses using hierarchical transfer learning to improve exploration in new environments, but this is useful mostly for sparse reward environments.
    In contrast, we are not interested in addressing sparse rewards, but rather in (qualitative) performance on the difficult locomotion task.
    That is, we should feel free to make the rewards as dense and as shaped as we want.
    The reward signal is a tool, not something given to us a priori.

  \item \cite{hausman2018learning} is an interesting theoretical paper showing how to get diverse meta-controls without imposing much structure on the interface between the HLC and the LLC.
    However, in our case, we have a prior idea about a good hierarchical breakdown of the tasks (namely, a footstep plan).
    \nhatch{I had some trouble with this paper; I should probably reread it and update this summary.}

  \item \cite{daume18ilrl} consider using hierarchical policies to improve the query efficiency of interactive imitation learning.
    In contrast, our project does not make use of an expert.
    Even if we do end up using motion capture, such an expert is not hierarchical, and it does not specify actual actions, just target poses.
    However, perhaps for some of the tasks in our curriculum, we will use a hand-coded expert for the HLC in order to pretrain the LLC.

  \item \cite{vezhnevets2017feudal} \emph{learns} diverse low-level policies using a latent state space and directional goals.
    However, we would like a more explicit hierarchical interface in the form of a footstep plan, and our low-level action space is continuous rather than discrete.

  \item \cite{frans2018meta} uses HRL for transfer learning, but they do not apply their results to curriculum learning.
    Hence, the tasks they accomplish remain fairly simple (simple maze traversal, etc.).

  \item \cite{kulkarni2016hierarchical} uses hierarchical RL to encourage exploration in sparse-reward environments.
    They also only consider discrete action spaces; perhaps we don't need to cite this paper at all.
  \item \cite{nachum2018data}: This paper tries to do off-policy hierarchical RL by changing the actions from the transitions stored in the high-level replay buffer.
    This seems like a hacky way to do things, and their experiments are clearly gaming the system to make their method look good.
    I don't think we should cite this.
\end{itemize}

\subsection*{Curriculum and transfer learning for locomotion}

\begin{itemize}
  \item \cite{karpathy2012curriculum} use a two-level hand-designed curriculum to achieve impressive acrobatic results for a low-dimenisonal hopper.
    However, their method involves random search and nonparametric nearest-neighbor policies, which are unlikely to generalize to higher-dimensional morphologies.

  \item The aforementioned paper \cite{heess2017emergence} trains their policies using curriculum learning: the environment gets progressively more difficult as time goes on.
    This achieves great results, but they do not quite succeed on bumpy ground.
    In this project, we hope to explore hierarchical policies as a potential solution to this difficult task.

  \item As mentioned before, \cite{peng2018deepmimic} uses some curriculum learning to accomplish the ``varied terrain'' tasks.

  \item As mentioned before, \cite{frans2018meta} has the explicit goal of using HRL for transfer learning.

  \item Relay Nets \citep{kumar2017relay} are a form of policy switching using the value function to choose between an ensemble of policies trained on different starting distributions, which uses a form of curriculum learning to train policies sequentially.
    In contrast, we wish to train a single policy while varying the environment itself.

\end{itemize}

\subsection*{Locomotion policy architectures}

\begin{itemize}
  \item At the lowest level, we will probably be using some kind of stable PD controller, in order to make the reinforcement learning problem as easy as possible.
    \nhatch{I need to read these papers:} \cite{tan2011stable}, \cite{shrivastava2013stable}
  \item \cite{peng2015terrain} introduces a very interesting way of parametrizing the action space in terms of reference points for PD controllers, as well as ``virtual forces''.
    We will want to do something similar.
  \item We might want to use something simple, like a linear controller as proposed by \cite{mania2018simple} to see if that works well.
    It seems to do a good job on OpenAI benchmarks, at least.
  \item Otherwise, we'll probably end up using TRPO or PPO or something (get citation later\dots)
  \item Other things to consider: prioritized experience replay, hindsight experience replay, universal value function approximators.
\end{itemize}


\subsection*{Model-based locomotion}

\begin{itemize}
  \item \cite{de2010feature} introduce a method for specifying locomotion behaviors with high-level features.
    From these features, low-level torques can be derived automatically by solving a series of quadratic programs based on physical models.
    This method is somewhat slow (50\% real time), and we might be able to avoid using hand-engineered models by employing deep networks.

  \item \cite{mordatch2010robust} use \cite{de2010feature} to execute a footstep plan which is generated using a lower-dimensional simplified physics model (a spring-loaded inverted pendulum).
    We might be able to use such an optimization- / model-based method as an expert, if we want to employ some kind of imitation learning.
    This could be similar to the approach of AlphaGo Zero.

  \item \cite{mordatch2012discovery} handles contacts in a clever way, using piecewise-constant, smooth contact variables and a continuation scheme.
    This allows complex motions to emerge from a quadratic optimization procedure.
    However, this method is very slow (2\% real time) and it generates only static motion (no jumping or slipping).
    Again, maybe we could use it as an expert.

\end{itemize}

\section{Project summary}

\begin{enumerate}

\item \textbf{One-sentence summary:}
Learn a difficult locomotion task with curriculum learning, hierarchical policies, and possibly model-based planning.

\item \textbf{Task:}
Train a simulated robotic agent to accomplish locomotion in a difficult environment, such as walking (or preferably running) across bumpy ground.

\item \textbf{Introduction:} Robotic locomotion on uneven terrain is an important problem.
  \nhatch{Visak, I would be interested in your high-level reasons for being interested in this problem.
  I'm not sure I can motivate the problem as well as you can.} Previous approaches have not solved this problem.
  For instance, in \href{https://www.youtube.com/watch?v=hx_bgoTF7bs&feature=youtu.be&t=1m28s}{this video} the agent stumbles on rubble, and DeepLoco only walks on flat terrain.
  \nhatch{More examples would help, to make sure that this problem is interesting.} We hope to achieve efficient locomotion across bumpy ground using a combination of three techniques.
  \begin{enumerate}
    \item Curriculum learning: Train the agent first on simpler environments, like flat ground and gradual slopes, and gradually increase the difficulty.
    \item Hierarchical controllers: We use a hierarchical architecture where the HLC plans foot placement and the LLC executes the individual steps.
      We hypothesize that this architecture will more easily handle the discontinuities caused by contact dynamics.
      We also hypothesize that, with this architecture, it will be easier to transfer knowledge between tasks in the curriculum.
    \item Model-based planning: To help with sample efficiency, the HLC has access to a physics model for the environment.
      \nhatch{I'm not sure how to accomplish this.
      Physics models for legged robots are probably hard, especially when the ground is not flat.
      Maybe some kind of \emph{learned} model would work.}
  \end{enumerate}

\item \textbf{Controller architecture:}
  Like DeepLoco, the HLC receives detailed environmental observations, and outputs the target locations for the next couple of foot placements.
  The LLC takes this input to choose the low-level actions that will achieve those foot placements.
  Unlike DeepLoco, the terrain is much more complicated, so it is important that the HLC foot placements are actually feasible.
  Furthermore, we will train the LLC without using motion captures, and we will permit the time per step to vary, rather than requiring each step to take exactly 0.5 seconds.

  The parametrization of the HLC and LLC will probably end up being some kind of neural network, although it might be worth trying simpler policies first (e.g. linear).

  To train these controllers, we could first train the LLC on simpler terrain, where we can specify target foot placement using a simple calculation.
  When training the HLC, to ensure that it chooses feasible foot placements, we could include some additional loss term in the training objective based on whether the LLC actually achieved the foot placement that the HLC suggested.

\nhatch{This section could probably use more detail; suggestions are welcome.
For example, do we need to modify this architecture to better handle contact dynamics?
Is there some way we can incorporate a model to help the HLC make better plans?}

\item \textbf{Why this might be a good idea:}
I have not seen previous papers succeed at this task, so if we could show some nice videos of a quadruped or biped running along bumpy ground, it would catch people's attention.
Intuitively, when running along bumpy hiking trails, I am consciously thinking about where to place my feet, then relying on unconsciously being able to place my foot there while I think about the next step.
This suggests a kind of hierarchical policy.

\item \textbf{Why this might not be a good idea:}
  \nhatch{Why would *hierarchical* RL be a particularly good way to do transfer learning for this series of tasks?}
  We will need to implement several other baselines to compare against.
  \nhatch{What should those baselines be? (What other approaches have people used for locomotion on bumpy ground?)
Is the ``bumpy ground'' task too difficult? (Byron thinks so.)
Will the simulated robot need toes?}
I haven't worked with locomotion before, so I don't have a good sense for this.

\end{enumerate}


\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
