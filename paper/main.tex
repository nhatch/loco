\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage[round]{natbib}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\sep}{sep}
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{red},
  stringstyle=\color{red},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\fig}[1]{\includegraphics[width=0.48\textwidth]{#1}}
\newcommand{\figb}[1]{\includegraphics[width=0.95\textwidth]{#1}}
\newcommand{\figz}[1]{\includegraphics[width=0.23\textwidth]{#1.png}}
\newcommand{\m}[1]{\mathbf{#1}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\TT}{\mathcal{T}}

\newcommand{\nhatch}[1]{{\leavevmode\color{blue} Nathan: #1}}

\usepackage{fancyhdr}
\pagestyle{fancy}

\title{Curriculum and Imitation Learning for Biped Locomotion Over Rough Terrain}
\author{TBD}

\begin{document}

\maketitle

\section{Introduction}

Locomotion is a difficult, underactuated control problem.
\emph{Biped} locomotion is more difficult because the support polygon is small and the center of mass is relatively high.
Biped locomotion across \emph{rough terrain} is more difficult still, because the variability in the surface normal makes the dynamics harder to model.

One way to handle these challenges is with detailed models and extensive optimization.
For example, the WPI-CMU team in the DARPA Robotics Challenge (DRC) controlled ATLAS using a hierarchical optimization approach that carefully controlled the robot's center of mass (CoM) and center of pressure (CoP) \citep{feng2015optimization}.
Such approaches tend to be reliable but slow.
They are physically slow because accurate modeling requires relatively static walking, and also because the optimization procedures are computationally slow.
For the DRC, reliability was more important than speed, but eventually we want to have robots that can traverse rough terrain using faster, dynamic gaits.

Recent advances in reinforcement learning (RL) suggest an alternative approach.
Deep RL is a popular technique to learn highly dynamic motions for humanoids \citep{peng2018deepmimic, heess2017emergence}.
The resulting policies are both computationally and physically fast.
However, their effectiveness on rough terrain is uncertain.

Building on this work, we approach dynamic biped locomotion on rough terrain using a combination of curriculum learning and imitation learning, which we call ResCu. \nhatch{Just a working title.}
\nhatch{Should mention that the ``controller'' below is only learning a residual on top of an already-reassnable basic controller.}
The controller learns a fast policy through imitation of an expert that uses (slow) zero-order optimization.
Use of a curriculum allows the controller to build up from flat ground to a challenging combination of stepping stones at various heights, distances, and orientations.
In contrast to prior work in learning-based locomotion, our controller \emph{plans its footstep locations}, which allows it to handle larger and more frequent obstacles.
In contrast to prior work in footstep planning, our controller learns a dynamic policy that requires no optimization at execution time.

We evaluate this algorithm on a simulated Darwin humanoid robot.
Of course, applying the resulting policy to a real-world system requires overcoming the ``reality gap''.
Nonetheless, recent advances in sim-to-real techniques \citep{tan2016simulation, tan2018sim} make us optimistic that ResCu will be a valuable technique in robotic biped locomotion.

\section{Related work}

\subsection{Step planning}

Many approaches to locomotion on physical robots use optimization-based planning with detailed models.
These approaches tend to generate \emph{hierarchical} plans in order to reduce computational requirements.
First, they choose footstep locations using search, like A*.
Based on the footstep plan, they generate a CoM trajectory with a simplified physics model such as a spring-loaded inverted pendulum \citep{mordatch2010robust} or contact wrench cone \citep{dai2016planning}.
Then they use inverse kinematics to plan trajectories for end-effectors \citep{zucker2010optimization}.
Finally, these trajectories are optimized using the full dynamics model to account for obstacles and other details \citep{ratliff2009chomp}.

These approaches tend to be reliable but slow \nhatch{how slow? find an example / citation}, because static walking is easier to model and optimization takes a lot of computation time.
They also require a lot of domain knowledge and manual tuning in order to engineer good models and handle situations when the real world deviates from ideal physics.
For now, they are the best known approaches to robotic locomotion, given the supreme importance of safety and reliability for large physical systems and high-stakes competitions such as the DARPA Robotics and Learning Locomotion Challenges.
However, we are interested in exploring alternative possibilities that can exploit the dynamics of the system to achieve faster and more energy-efficient gaits.

\nhatch{I think Honda does a lot with dynamic gaits, but I don't really understand how it works. I need to find and read some papers. To what extent can they plan step locations and handle rough terrain?}

There are a few prior works that use learning-based approaches to do step planning for simulated systems.
\cite{peng2017deeploco} use a hierarchical reinforcement learning algorithm where the upper-level controller chooses footstep locations and the lower-level controller tries to execute those plans.
However, the target step locations are unrealistic and rarely actually achieved.
The path that the agent follows is perfectly flat; on rough terrain, such inaccurate step planning would probably fail.
\cite{karpathy2012curriculum} use curriculum learning for a simple planar biped to collect a set of (state, action, resulting step distance) tuples.
However, the final policy is based on nearest-neighbor search among this set of collected experience.
It would be challenging to apply this approach to a higher-dimensional, more complicated model.

\subsection{Deep reinforcement learning}

Most of the prior work on deep RL for biped locomotion over rough terrain does not explicitly plan individual footstep locations.
Examples of this are \cite{peng2018deepmimic, heess2017emergence, peng2016terrain}. The resulting policies travel forward quickly, avoid certain obstacles, and are robust to small missteps.
However, they have not been proven to work in situations with dense or large obstacles where small missteps can be catastrophic.
Such environments require accurate predictive planning at the level of individual contacts.

\section{Our approach}

In contrast to previous work, our approach requires no optimization during policy execution, but it is still able to land the swing heel at a prespecified target location.
For now, we assume that perception is solved\footnote{In the experiments, we evaluate the robustness of our algorithm with respect to errors in the perception system.} and that the step plan (in the form of a list of target step locations) is given by an oracle.
The problem is to choose motor torques to accomplish the given step plan.


Our algorithm is called ResCu, which stands for ``residual curriculum learning''.
It consists of four components: a basic parametrized walking controller, a higher-level controller that outputs parameters for the basic controller, a data collection scheme for single-step targets according to a gradually more difficult curriculum, and an optimization algorithm to solve those single-step targets.
\nhatch{This system is rather complicated, and probably some of these components are unnecessary or unwise to include in the current paper. A major focus of work in August will be ablation studies to determine which of these ideas are most important.}

\subsection{Basic walking controller}

In order to simplify the learning problem as much as possible, we engineer and manually tune a walking controller that, with no learning, can walk forward and to some extent control the length of its steps.
While this controller could in principle be learned using RL techniques, our approach has several advantages.
First, the basic walking gait looks fairly natural, in contrast to most results from deep RL.
Second, it has an interpretable set of parameters which provide a convenient interface for another, higher-level controller.
Third, because the basic controller already produces reasonable results, the higher-level controller can learn a residual rather than an entire policy.
This allows us to use simpler, more sample-efficient function approximators, and it reduces the computational complexity of the learning algorithm.

Our basic controller is a modification of the walking gait from SIMBICON \citep{yin2007simbicon}.
SIMBICON uses a finite state machine (FSM) with two target poses: an UP state that ends after 0.3 seconds, and a DOWN state that ends on swing foot contact.
During each state, proportional derivative (PD) controllers attempt to reach the corresponding target pose.
Balance feedback is achieved by adjusting the swing hip angles at each time step based on the distance and velocity of the agent's CoM relative to its stance foot contact location.
After hand-tuning the balance feedback gains and the joint angles for each state, this simple control scheme can walk forward and maintain its balance.

We make several novel modifications to SIMBICON in order to control its step length:
\nhatch{Might want to put some of this in the appendix instead. Also, using some equations might make the writing more precise.}
\begin{enumerate}
  \item At the beginning of each step, set the swing hip, swing knee, stance knee, and stance ankle angles of the UP state as a linear function of the distance to the next step target.
    The coefficients of this function were manually tuned such that the resulting step distance approximately matches the target distance.

  \item End the UP state only when both the swing heel and the CoM are within a certain distance of the target step location.
    This distance is proportional to the CoM speed, where the proportionality constant (gain) is an additional parameter.
    (If the swing heel hits the ground early, the step immediately ends.)
    Getting the swing heel close enough to the target is important to ensure that the DOWN state accurately hits the target.
    Getting the CoM close enough is important to ensure that the heel strike will propel the robot forward rather than backward.
    (This is especially important for long steps.)

  \item Automatically set the swing leg pose in the DOWN state using inverse kinematics (IK).
    Again, adjust the target swing heel location for the purposes of IK from the true target location based on the CoM speed and an additional gain parameter.

  \item Control the swing ankle angle in world space, rather than relative to the shin.
    This does not have much effect on the step distance, but it helps make the dynamics of each impact more predictable, which is important for accurate function approximation.
    \nhatch{Again, all of this will have to be validated with ablations\dots.}
\end{enumerate}

%This choice simplifies the dynamics that the lower level needs to model, because each step involves no (or very few) changes of contact.

\subsection{Higher-level controller}

The basic controller can walk and balance on flat ground, and to some extent control its step length.
However, it cannot handle stairs and inclines, and when the step distance is irregular, it cannot hit the targets with high accuracy.
\nhatch{Get some numbers about how big the steps/inclines have to be before problems happen, how bad the accuracy is, etc.}
Hence, we train a higher-level controller (HLC) on top of the basic controller.
It is queried once at the beginning of each step.
It has the same input (``state'') as the basic controller: joint angles and velocities, stance heel location, and target step location (46 dimensions in 3D, 22 dimensions in 2D).
\nhatch{The basic controller also observes the robot's center of mass. Not sure where to mention this, or whether it will actually be necessary in the final version.}
Its output (``action'') is a vector of parameters that modify the behavior of the basic controller during the subsequent step (22 dimensions in 3D, 18 dimensions in 2D; see Table \ref{table:params}).
These parameters are the same parameters that were hand-tuned during the initial design of the basic controller; the HLC output corresponds to a residual on top of the hand-tuned values.
We exploit bilateral symmetry by mirroring the joint states if necessary so that the swing foot is always the right foot.

\begin{table}
  \caption{Parameters of the basic walking controller for the UP phase.
  The DOWN phase has the same set of eleven parameters (for 22 parameters total) except that \texttt{ik\_gain} now refers to the adjustment of the swing heel target used for IK.
  \nhatch{For many of these parameters, it is reasonable to set the same values for both the UP and DOWN phases. We might be able to reduce problem dimensionality by removing the mostly-redundant parameters.}}
  \label{table:params}
  \hrule
  \begin{tabular}{ll}
    \texttt{torso\_world} & Torso angle in world reference frame. \\
    \texttt{swing\_hip\_world} & Swing hip angle (sagittal plane) in world frame. \\
    \texttt{swing\_knee\_relative} & Swing knee angle relative to thigh.\\
    \texttt{swing\_ankle\_relative} & Swing ankle angle relative to thigh. \nhatch{Actually this is in world frame too?}\\
    \texttt{stance\_knee\_relative} & Stance knee angle relative to thigh.\\
    \texttt{stance\_ankle\_relative} & Stance ankle angle relative to shin.\\
    \texttt{position\_balance\_gain} & Gain for sagittal swing hip angle based on position.\\
    \texttt{position\_balance\_gain\_lat} & Same as above for the coronal angle.\\
    \texttt{velocity\_balance\_gain} & Gain for sagittal swing hip angle based on velocity.\\
    \texttt{velocity\_balance\_gain\_lat} & Same as above for the coronal angle.\\
    \texttt{ik\_gain} & Gain for distance threshold that triggers the end of the UP phase.
  \end{tabular}
  \hrule
\end{table}


We implement the HLC by training a parametrized function approximator on a set of collected experience.
This dataset is a set of (state, action, actual achieved footstep location) tuples, collected as detailed in Sec. \ref{sec:curriculum} and \ref{sec:optimization}.
Note however that we want the HLC to output an action.
Hence, for the regression task, the input is the pair (state, actual achieved footstep location) and the output is the action.
Note that the input includes two ``target inputs'': the target swing heelstrike location, which was originally used when collecting the sample, and the actual swing heelstrike location, which was where the heel actually hit the ground.
When using the HLC to prescribe actions given a new state and target, we use the same target for both of the HLC's ``target inputs''.
\nhatch{Now that I write this out, that seems pretty unclean. I should probably make the ``target used when collecting the sample'' part of the \emph{action} instead. Unfortunately, that puts us in a situation where the HLC is setting ``targets'' for the basic controller that are not actually the targets that it wants the basic controller to hit. But I guess it means we could maybe remove the \texttt{ik\_gain} parameters.}

For this project, we chose a linear hypothesis class for the HLC.
\nhatch{Soon, I want to experiment with more complicated hypothesis classes.}
Because our dataset is relatively small, we use ridge regression to avoid overfitting.
Our dataset also tends to contain outliers, because the basic controller can behave unpredictably in some situations (such as when the agent stubs its toe).
We handle this by using RANSAC \citep{fischler1981random} to discard outliers at training time.
\nhatch{RANSAC might not be necessary if the optimization alg. to collect the dataset is good enough. Maybe that alg could throw out examples where no good parameter setting was found.}

\subsection{Curriculum} \label{sec:curriculum}

We collect the dataset in an iterative, aggregate way, similar to DAgger \citep{ross2011reduction}.
First, we collect an initial dataset by running the basic controller for several steps on flat ground with a variety of different target step lengths.
On subsequent rounds of data collection, we proceed as follows:
\nhatch{Format this as an algorithm.}
\begin{enumerate}
  \item Choose $N$ starting states from the existing dataset.
  \item For each starting state, choose $M$ target locations for the next step.
  \item For each target location, find an action to achieve that target using random optimization (see Sec. \ref{sec:optimization}).
  \item For each of $P$ Gaussian perturbations of the optimal action from step 3, run the simulation and add the sample to the dataset.
\end{enumerate}
For our experiments, we used $N=128$, $M=1$, $P=4$.
The optimization in step 3 is rather expensive, hence the relatively small number of samples $NMP$ collected at each round.
\nhatch{I'm not sure that the Gaussian perturbations are actually helpful. If they are, it might make sense to perturb the starting state as well, not just the action.}

Note that the starting states for each round are chosen from the set of ending states collected during all previous rounds.
Hence, the input distribution of the dataset is indeed representative of the distribution of states likely to be seen by the HLC at test time.

Finally, we incorporate an element of curriculum learning into the data collection process.
At certain manually specified points in the process, we change the distribution of targets used in step 2 and/or change some details of the simulation.
For instance, at first, the targets are all chosen to lie in the ground plane, and the variance in their distance from one another is relatively small.
Once the HLC reaches a desired level of accuracy, the variance is increased.
After that, we change the environment such that the ground is no longer a single flat sheet, but rather a series of stepping stones with gaps in between them.
Later still, we introduce some vertical variation into the target step locations.\footnote{The stepping stones are shifted up or down to match the target location.}.
\nhatch{And so forth. I haven't systematically studied this yet to see how difficult we can get.}

We believe \nhatch{and test experimentally} that this curriculum of gradually increasing difficulty is key to good performance.
In our algorithm, the initial point of the optimization problem in step 3 is the action suggested by the current HLC.
The problem is nonconvex, so this initialization point matters a lot.
With better initialization, the stationary point is more likely to be a good local optimum, and the optimization problem might be more quickly solvable.
Hence, it makes sense to scale the difficulty of the optimization problem roughly proportionally to the current capabilities of the HLC.

\subsection{Single-step optimization} \label{sec:optimization}

We now give more details about the optimization problem in step 3 of Sec. \ref{sec:curriculum}.
The objective of the problem is the Euclidean distance between the target and achieved step locations.
We define the achieved step location as the location of the swing heel at the moment that the step ends, which is the moment that any part of the swing foot makes contact with the ground.\footnote{The step also ends if the robot crashes; i.e. falls off of the edge of a stepping stone, falls over, etc.}
The optimization variables are the parameters of the basic controller (see Table \ref{table:params}).
There are no constraints.
\nhatch{Actually, we constrain most of the parameters to be zero. In my experiments so far, only seven of the 22 parameters are allowed to vary. It might be possible to gradually allow more parameters to be controlled according to the requirements of the curriculum.}

Because it would be difficult to calculate gradients for this objective, we use the zero-order optimization algorithm described in \cite{mania2018simple}.
At each iteration, we estimate gradients as a weighted average of $Q = 4$ random directions, with the weight for each direction chosen as the central difference of the objective value along that direction.
Hence, we perform $2Q$ single-footstep rollouts for each update of the optimization variables.
We end the optimization once the accuracy reaches two centimeters, or once a certain maximum number of iterations (in our case, ten) is reached.

\section{Experiments}

We perform experiments with a series of three robot models of increasing realism: (1) a simple planar biped, (2) a simple 3D biped, and (3) a model of the Darwin robot.
All experiments are performed using the DART simulator at a simulation rate of 2000 Hz.

\nhatch{This section is still pretty rough; sorry.}
Our experiments are designed to test two hypotheses.
Our primary hypothesis is that this combination of imitation and curriculum learning can learn to navigate a variety of complicated locomotion environments, learning faster and achieving better results than other reinforcement learning methods.

We collect both quantitative and qualitative metrics to measure ``achieving better results''.
Quantitatively, we evaluate the robot by presenting it with a series of small stepping stones placed at a random $(x,y,z)$ offsets from each other.
\nhatch{It would be nice to be able to place them at different rotations/orientations as well.}
We score the robot based on a combination of the number of successful steps before crashing and the accuracy of foot placement at each step.
We collect this score for a number of different distributions of the $(x,y,z)$ offsets (of increasing difficulty), and find \nhatch{hopefully} that our model achieves better performance than deep RL methods on most of these environments.
\nhatch{We will have to think carefully about how to make this a fair comparison, since most deep RL methods don't actually care about foot placement. For one thing, the action space for the deep RL methods will also have to be ``parameter settings for the basic walking controller''.}
We also consider the robustness of the model to inaccurate perception by perturbing the target step locations with respect to the actual location of the stepping stones.
\nhatch{We should probably also evaluate robustness to external force perturbations etc., but I'm nervous that that won't end well for us. We don't really have good feedback mechanisms \emph{during} each footstep.}
Qualitatively, we document motion quality by means of supplementary videos, which we encourage the reader to consult.

Our two metrics for ``learning faster'' are (1) number of simulation frames of experience collected, and (2) total computation time.
We consider both necessary, because the relative expense of more experience versus more computation may vary across application domains.

Our secondary hypothesis is that the learned policy can be run in real-time while still achieving results comparable to more expensive, model-based optimization methods.
This hypothesis is difficult to test, because model-based optimization methods are difficult to implement for direct comparison.
Thus, our metric is simply the percent real time necessary to compute the control for the next time step, which we compare to numbers reported in optimization-based papers.

Finally, we conduct ablative analysis of the importance of various components of our approach.
\nhatch{This is super important given the current complexity of the system. We need to remove some of these elements to make a more focused paper, so we need to identify what matters and what doesn't.
For reference, a list of the various components that we want to pare down: the many modifications to Simbicon, the complexity of the curriculum (and whether a curriculum is necessary at all), the hypothesis class for the HLC, the data collection algorithm from Sec. \ref{sec:curriculum} (e.g. do Gaussian perturbations help), the set of parameters that are controllable by the HLC, whether RANSAC is necessary, whether the basic controller observes the CoM or not.}

\section{Discussion}

We see at least two important directions for future work.
\begin{enumerate}
  \item \emph{Sim-to-real:}
    We have successfully applied CIRL to a realistic simulated model, but it is likely that more work is required to transfer the resulting policy to the real world.
  \item \emph{Motion capture:}
    A significant portion of the effort for this project went into developing a reasonable baseline walking policy.
    It might be possible to avoid this by bootstrapping the imitation learning process with motion capture data.
    This would also make it easier to to extend CIRL to other robotic motion problems, such as manipulation.
\end{enumerate}

\nhatch{Miscellaneous TODO notes for me:
\begin{enumerate}
  \item Set damping coefficients for the DART simulation. Visak says this will help prevent numerical explosions during collisions.
  \item Get Simbicon3D to be able to roughly control step distances, similar to 2D Simbicon. Or maybe this isn't necessary? Maybe we could jump straight to the learning algorithm?
  \item Conduct ablation analysis to try to reduce the number of ``new ideas'' presented in this paper.
  \item Think about how best to compare this algorithm to existing work (both RL work and traditional optimization-based step planning).
\end{enumerate}
}

\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
