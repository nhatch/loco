\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage[round]{natbib}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\sep}{sep}
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{red},
  stringstyle=\color{red},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\fig}[1]{\includegraphics[width=0.48\textwidth]{#1}}
\newcommand{\figb}[1]{\includegraphics[width=0.95\textwidth]{#1}}
\newcommand{\figz}[1]{\includegraphics[width=0.23\textwidth]{#1.png}}
\newcommand{\m}[1]{\mathbf{#1}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\TT}{\mathcal{T}}

\newcommand{\highlight}[1]{{\leavevmode\color{blue} #1}}
\newcommand{\warning}[1]{{\leavevmode\color{red}Warning: #1}}

\usepackage{fancyhdr}
\pagestyle{fancy}

\title{Curriculum and Transfer Learning of Hierarchical Locomotion Policies}
\author{TBD}

\begin{document}

\maketitle

Definitions:
\begin{itemize}
  \item HLC: high-level controller
  \item LLC: low-level contoller
  \item HRL: hierarchical reinforcement learning
\end{itemize}

\section{Literature review}

\subsection*{Locomotion on varied terrain}

\begin{itemize}
  \item \cite{peng2016terrain}: Perhaps the most closely related prior work, this paper trains planar characters to run across varied terrain. In the discussion section of this paper, they mention three specific directions for extension that essentially exactly define our current project: move to three dimensions, consider more difficult terrains (smaller, more frequent obstacles) that require footstep planning, and use curriculum learning.

  \item \cite{heess2017emergence} trains policies that can, to some extent, handle bumpy ground (see \href{https://www.youtube.com/watch?v=hx_bgoTF7bs&feature=youtu.be&t=1m28s}{this video}).
    However, the agents here do not actually observe the bumps.
    They assume flat ground and depend on policy robustness to handle unexpected terrain variation.
    They do not report quantitative results.
    This approach is likely limited in the kinds of bumps that it can handle.
    In the video, the humanoid trips.

  \item \cite{peng2017deeploco}
    learns locomotion in an environment that includes bumpy ground, but the path they follow is perfectly flat.
    They also use motion capture to train the LLC, and the steps are all of uniform length (0.5 seconds), which we hope to avoid.

  \item \cite{peng2018deepmimic} includes some tasks that involve running across varied terrain.
    They also use a form of curriculum learning, where the policies are first trained on flat ground (and without a vision component), before training then continues on the full problem.
    Their policies are somewhat hierarchical, in the sense that they output a target pose, which is then used as input for a PD controller to generate torques.
    Qualitatively, the policies on these tasks seem to mostly ignore the obstacles, relying on being able to recover from the perturbations generated by collisions.
    In contrast, we would like to avoid the kind of stumbling seen in these videos by introducing another level of hierarchy that generates an explicit footstep plan.


  \item \cite{peng2017learning} uses bumpy ground to test robustness of learned policies, but again the bumps are not actually perceived and cannot be planned around. The bump height thus must remain small relative to the agent. \highlight{We should compare this quantitatively, later.} Also, this is 2D.
  \item \cite{manchester2011stable} This paper stands out in that they derive a provably stable controller that they test on a physical robot. \highlight{I haven't read it in detail yet, though.}
\end{itemize}

\subsection*{Hierarchical reinforcement learning}

\begin{itemize}
  \item \cite{heess2016learning} discusses using hierarchical transfer learning to improve exploration in new environments, but this is useful mostly for sparse reward environments.
    In contrast, we are not interested in addressing sparse rewards, but rather in (qualitative) performance on the difficult locomotion task.
    That is, we should feel free to make the rewards as dense and as shaped as we want.
    The reward signal is a tool, not something given to us a priori.

  \item \cite{hausman2018learning} is an interesting theoretical paper showing how to get diverse meta-controls without imposing much structure on the interface between the HLC and the LLC.
    However, in our case, we have a prior idea about a good hierarchical breakdown of the tasks (namely, a footstep plan). \highlight{I had some trouble with this paper; I should probably reread it and update this summary.}

  \item \cite{daume18ilrl} consider using hierarchical policies to improve the query efficiency of interactive imitation learning.
    In contrast, our project does not make use of an expert.
    Even if we do end up using motion capture, such an expert is not hierarchical, and it does not specify actual actions, just target poses.
    However, perhaps for some of the tasks in our curriculum, we will use a hand-coded expert for the HLC in order to pretrain the LLC.

  \item \cite{vezhnevets2017feudal} \emph{learns} diverse low-level policies using a latent state space and directional goals. However, we would like a more explicit hierarchical interface in the form of a footstep plan, and our low-level action space is continuous rather than discrete.

  \item \cite{frans2018meta} uses HRL for transfer learning, but they do not apply their results to curriculum learning.
    Hence, the tasks they accomplish remain fairly simple (simple maze traversal, etc.).

  \item \cite{kulkarni2016hierarchical} uses hierarchical RL to encourage exploration in sparse-reward environments.
    They also only consider discrete action spaces; perhaps we don't need to cite this paper at all.
  \item \cite{nachum2018data}: This paper tries to do off-policy hierarchical RL by changing the actions from the transitions stored in the high-level replay buffer. This seems like a hacky way to do things, and their experiments are clearly gaming the system to make their method look good. I don't think we should cite this.
\end{itemize}

\subsection*{Curriculum and transfer learning for locomotion}

\begin{itemize}
  \item \cite{karpathy2012curriculum} use a two-level hand-designed curriculum to achieve impressive acrobatic results for a low-dimenisonal hopper. However, their method involves random search and nonparametric nearest-neighbor policies, which are unlikely to generalize to higher-dimensional morphologies.

  \item The aforementioned paper \cite{heess2017emergence} trains their policies using curriculum learning: the environment gets progressively more difficult as time goes on.
    This achieves great results, but they do not quite succeed on bumpy ground.
    In this project, we hope to explore hierarchical policies as a potential solution to this difficult task.

  \item As mentioned before, \cite{peng2018deepmimic} uses some curriculum learning to accomplish the ``varied terrain'' tasks.

  \item As mentioned before, \cite{frans2018meta} has the explicit goal of using HRL for transfer learning.

  \item Relay Nets \citep{kumar2017relay} are a form of policy switching using the value function to choose between an ensemble of policies trained on different starting distributions, which uses a form of curriculum learning to train policies sequentially.
    In contrast, we wish to train a single policy while varying the environment itself.

\end{itemize}

\subsection*{Locomotion policy architectures}

\begin{itemize}
  \item At the lowest level, we will probably be using some kind of stable PD controller, in order to make the reinforcement learning problem as easy as possible. \highlight{I need to read these papers:} \cite{tan2011stable}, \cite{shrivastava2013stable}
  \item \cite{peng2015terrain} introduces a very interesting way of parametrizing the action space in terms of reference points for PD controllers, as well as ``virtual forces''. We will want to do something similar.
  \item We might want to use something simple, like a linear controller as proposed by \cite{mania2018simple} to see if that works well.
    It seems to do a good job on OpenAI benchmarks, at least.
  \item Otherwise, we'll probably end up using TRPO or PPO or something (get citation later\dots)
  \item Other things to consider: prioritized experience replay, hindsight experience replay, universal value function approximators.
\end{itemize}


\subsection*{Model-based locomotion}

\begin{itemize}
  \item \cite{de2010feature} introduce a method for specifying locomotion behaviors with high-level features. From these features, low-level torques can be derived automatically by solving a series of quadratic programs based on physical models. This method is somewhat slow (50\% real time), and we might be able to avoid using hand-engineered models by employing deep networks.

  \item \cite{mordatch2010robust} use \cite{de2010feature} to execute a footstep plan which is generated using a lower-dimensional simplified physics model (a spring-loaded inverted pendulum). We might be able to use such an optimization- / model-based method as an expert, if we want to employ some kind of imitation learning. This could be similar to the approach of AlphaGo Zero.

  \item \cite{mordatch2012discovery} handles contacts in a clever way, using piecewise-constant, smooth contact variables and a continuation scheme. This allows complex motions to emerge from a quadratic optimization procedure. However, this method is very slow (2\% real time) and it generates only static motion (no jumping or slipping). Again, maybe we could use it as an expert.

\end{itemize}

\section{Project summary}

\begin{enumerate}

\item \textbf{One-sentence summary:}
Learn a difficult locomotion task with curriculum learning, hierarchical policies, and possibly model-based planning.

\item \textbf{Task:}
Train a simulated robotic agent to accomplish locomotion in a difficult environment, such as walking (or preferably running) across bumpy ground.

\item \textbf{Introduction:} Robotic locomotion on uneven terrain is an important problem. \highlight{Visak, I would be interested in your high-level reasons for being interested in this problem. I'm not sure I can motivate the problem as well as you can.} Previous approaches have not solved this problem. For instance, in \href{https://www.youtube.com/watch?v=hx_bgoTF7bs&feature=youtu.be&t=1m28s}{this video} the agent stumbles on rubble, and DeepLoco only walks on flat terrain. \highlight{More examples would help, to make sure that this problem is interesting.} We hope to achieve efficient locomotion across bumpy ground using a combination of three techniques.
  \begin{enumerate}
    \item Curriculum learning: Train the agent first on simpler environments, like flat ground and gradual slopes, and gradually increase the difficulty.
    \item Hierarchical controllers: We use a hierarchical architecture where the HLC plans foot placement and the LLC executes the individual steps. We hypothesize that this architecture will more easily handle the discontinuities caused by contact dynamics. We also hypothesize that, with this architecture, it will be easier to transfer knowledge between tasks in the curriculum.
    \item Model-based planning: To help with sample efficiency, the HLC has access to a physics model for the environment. \highlight{I'm not sure how to accomplish this. Physics models for legged robots are probably hard, especially when the ground is not flat. Maybe some kind of \emph{learned} model would work.}
  \end{enumerate}

\item \textbf{Controller architecture:}
  Like DeepLoco, the HLC receives detailed environmental observations, and outputs the target locations for the next couple of foot placements.
  The LLC takes this input to choose the low-level actions that will achieve those foot placements.
  Unlike DeepLoco, the terrain is much more complicated, so it is important that the HLC foot placements are actually feasible.
  Furthermore, we will train the LLC without using motion captures, and we will permit the time per step to vary, rather than requiring each step to take exactly 0.5 seconds.

  The parametrization of the HLC and LLC will probably end up being some kind of neural network, although it might be worth trying simpler policies first (e.g. linear).

  To train these controllers, we could first train the LLC on simpler terrain, where we can specify target foot placement using a simple calculation.
  When training the HLC, to ensure that it chooses feasible foot placements, we could include some additional loss term in the training objective based on whether the LLC actually achieved the foot placement that the HLC suggested.

\highlight{This section could probably use more detail; suggestions are welcome.
For example, do we need to modify this architecture to better handle contact dynamics?
Is there some way we can incorporate a model to help the HLC make better plans?}

\item \textbf{Why this might be a good idea:}
I have not seen previous papers succeed at this task, so if we could show some nice videos of a quadruped or biped running along bumpy ground, it would catch people's attention.
Intuitively, when running along bumpy hiking trails, I am consciously thinking about where to place my feet, then relying on unconsciously being able to place my foot there while I think about the next step.
This suggests a kind of hierarchical policy.

\item \textbf{Why this might not be a good idea:}
  \highlight{Why would *hierarchical* RL be a particularly good way to do transfer learning for this series of tasks?}
  We will need to implement several other baselines to compare against.
  \highlight{What should those baselines be? (What other approaches have people used for locomotion on bumpy ground?)
Is the ``bumpy ground'' task too difficult? (Byron thinks so.)
Will the simulated robot need toes?}
I haven't worked with locomotion before, so I don't have a good sense for this.

\end{enumerate}


\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}
