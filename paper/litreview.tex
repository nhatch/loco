\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\usepackage[round]{natbib}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\sep}{sep}
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{red},
  stringstyle=\color{red},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\fig}[1]{\includegraphics[width=0.48\textwidth]{#1}}
\newcommand{\figb}[1]{\includegraphics[width=0.95\textwidth]{#1}}
\newcommand{\figz}[1]{\includegraphics[width=0.23\textwidth]{#1.png}}
\newcommand{\m}[1]{\mathbf{#1}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\TT}{\mathcal{T}}

\newcommand{\nhatch}[1]{{\leavevmode\color{blue} Nathan: #1}}

\usepackage{fancyhdr}
\pagestyle{fancy}

\title{Curriculum and Imitation Learning for Biped Locomotion Over Rough Terrain}
\author{TBD}

\begin{document}

\maketitle

\section{Literature review}

\subsection*{Locomotion on varied terrain}

\begin{itemize}
  \item \cite{peng2016terrain}: Perhaps the most closely related prior work, this paper trains planar characters to run across varied terrain.
    In the discussion section of this paper, they mention three specific directions for extension that essentially exactly define our current project: move to three dimensions, consider more difficult terrains (smaller, more frequent obstacles) that require footstep planning, and use curriculum learning.

  \item \cite{zucker2010optimization} is a project from the DARPA Learning Locomotion challenge that uses optimization to generate hierarchical motion plans and recover from errors in real time.
    It has been tested on a physical robot with \href{https://www.youtube.com/watch?v=KpqFebvRLeQ}{impressive results}.
    We will need to compare our work carefully with this project in the final paper, and we will need to improve upon their results one way or another.
    The biggest difference might be in robot morphology / stability: executing step plans on bipeds is perhaps more difficult than executing step plans on quadrupeds given the small base of support during each step.
    Other avenues for improvement might be eliminating the need for 45 seconds of initial planning, traveling faster (running?), or using some kind of interesting new technique (e.g. reinforcement learning or deep learning).
    It is also worth considering whether our algorithm could work on a physical robot, though for the initial work we will unfortunately be working purely in simulation.

  \item \cite{heess2017emergence} trains policies that can, to some extent, handle bumpy ground (see \href{https://www.youtube.com/watch?v=hx_bgoTF7bs&feature=youtu.be&t=1m28s}{this video}).
    However, the agents here do not actually observe the bumps.
    They assume flat ground and depend on policy robustness to handle unexpected terrain variation.
    They do not report quantitative results.
    This approach is likely limited in the kinds of terrain that it can handle.
    In the video, the humanoid trips.

  \item \cite{peng2017deeploco}
    learns locomotion in an environment that includes bumpy ground, but the path they follow is perfectly flat.
    They also use motion capture to train the LLC, and the steps are all of uniform length (0.5 seconds), which we hope to avoid.

  \item \cite{peng2018deepmimic} includes some tasks that involve running across varied terrain.
    They also use a form of curriculum learning, where the policies are first trained on flat ground (and without a vision component), before training then continues on the full problem.
    Their policies are somewhat hierarchical, in the sense that they output a target pose, which is then used as input for a PD controller to generate torques.
    Qualitatively, the policies on these tasks seem to mostly ignore the obstacles, relying on being able to recover from the perturbations generated by collisions.
    In contrast, we would like to avoid the kind of stumbling seen in these videos by introducing another level of hierarchy that generates an explicit footstep plan.

  \item \cite{peng2017learning} uses bumpy ground to test robustness of learned policies, but again the bumps are not actually perceived and cannot be planned around.
    The bump height thus must remain small relative to the agent.
    \nhatch{We should compare this quantitatively, later.} Also, this is 2D.

  \item \cite{manchester2011stable} This paper stands out in that they derive a provably stable controller that they test on a physical robot.
    \nhatch{I haven't read it in detail yet, though.}

\end{itemize}

\subsection*{Hierarchical reinforcement learning}

\begin{itemize}
  \item \cite{heess2016learning} discusses using hierarchical transfer learning to improve exploration in new environments, but this is useful mostly for sparse reward environments.
    In contrast, we are not interested in addressing sparse rewards, but rather in (qualitative) performance on the difficult locomotion task.
    That is, we should feel free to make the rewards as dense and as shaped as we want.
    The reward signal is a tool, not something given to us a priori.

  \item \cite{hausman2018learning} is an interesting theoretical paper showing how to get diverse meta-controls without imposing much structure on the interface between the HLC and the LLC.
    However, in our case, we have a prior idea about a good hierarchical breakdown of the tasks (namely, a footstep plan).
    \nhatch{I had some trouble with this paper; I should probably reread it and update this summary.}

  \item \cite{daume18ilrl} consider using hierarchical policies to improve the query efficiency of interactive imitation learning.
    In contrast, our project does not make use of an expert.
    Even if we do end up using motion capture, such an expert is not hierarchical, and it does not specify actual actions, just target poses.
    However, perhaps for some of the tasks in our curriculum, we will use a hand-coded expert for the HLC in order to pretrain the LLC.

  \item \cite{vezhnevets2017feudal} \emph{learns} diverse low-level policies using a latent state space and directional goals.
    However, we would like a more explicit hierarchical interface in the form of a footstep plan, and our low-level action space is continuous rather than discrete.

  \item \cite{frans2018meta} uses HRL for transfer learning, but they do not apply their results to curriculum learning.
    Hence, the tasks they accomplish remain fairly simple (simple maze traversal, etc.).

  \item \cite{kulkarni2016hierarchical} uses hierarchical RL to encourage exploration in sparse-reward environments.
    They also only consider discrete action spaces; perhaps we don't need to cite this paper at all.
  \item \cite{nachum2018data}: This paper tries to do off-policy hierarchical RL by changing the actions from the transitions stored in the high-level replay buffer.
    This seems like a hacky way to do things, and their experiments are clearly gaming the system to make their method look good.
    I don't think we should cite this.
\end{itemize}

\subsection*{Curriculum and transfer learning for locomotion}

\begin{itemize}
  \item \cite{2018-ICLR-distill} This paper uses a similar curriculum-learning approach to 2D-biped locomotion on varied terrains, starting with flat ground and working up to a mixture of slopes, steps, and gaps. We may want to use their distillation approach when integrating multiple skills. (Alternatively, we could use what they call ``pure transfer learning'', but have each new task incorporate the relevant challenges of previous tasks.) They use motion capture, and the actions are target joint angles.

  \item \cite{karpathy2012curriculum} use a two-level hand-designed curriculum to achieve impressive acrobatic results for a low-dimenisonal hopper.
    However, their method involves random search and nonparametric nearest-neighbor policies, which are unlikely to generalize to higher-dimensional morphologies.

  \item The aforementioned paper \cite{heess2017emergence} trains their policies using curriculum learning: the environment gets progressively more difficult as time goes on.
    This achieves great results, but they do not quite succeed on bumpy ground.
    In this project, we hope to explore hierarchical policies as a potential solution to this difficult task.

  \item As mentioned before, \cite{peng2018deepmimic} uses some curriculum learning to accomplish the ``varied terrain'' tasks.

  \item As mentioned before, \cite{frans2018meta} has the explicit goal of using HRL for transfer learning.

  \item Relay Nets \citep{kumar2017relay} are a form of policy switching using the value function to choose between an ensemble of policies trained on different starting distributions, which uses a form of curriculum learning to train policies sequentially.
    In contrast, we wish to train a single policy while varying the environment itself.

\end{itemize}

\subsection*{Locomotion policy architectures}

\begin{itemize}
  \item At the lowest level, we will probably be using some kind of stable PD controller, in order to make the reinforcement learning problem as easy as possible.
    \nhatch{I need to read these papers:} \cite{tan2011stable}, \cite{shrivastava2013stable}
  \item \cite{peng2015terrain} introduces a very interesting way of parametrizing the action space in terms of reference points for PD controllers, as well as ``virtual forces''.
    We will want to do something similar.
  \item We might want to use something simple, like a linear controller as proposed by \cite{mania2018simple} to see if that works well.
    It seems to do a good job on OpenAI benchmarks, at least.
  \item Otherwise, we'll probably end up using TRPO or PPO or something (get citation later\dots)
  \item Other things to consider: prioritized experience replay, hindsight experience replay, universal value function approximators.
\end{itemize}


\subsection*{Model-based locomotion}

\begin{itemize}
  \item \cite{de2010feature} introduce a method for specifying locomotion behaviors with high-level features.
    From these features, low-level torques can be derived automatically by solving a series of quadratic programs based on physical models.
    This method is somewhat slow (50\% real time), and we might be able to avoid using hand-engineered models by employing deep networks.

  \item \cite{mordatch2010robust} use \cite{de2010feature} to execute a footstep plan which is generated using a lower-dimensional simplified physics model (a spring-loaded inverted pendulum).
    We might be able to use such an optimization- / model-based method as an expert, if we want to employ some kind of imitation learning.
    This could be similar to the approach of AlphaGo Zero.

  \item \cite{mordatch2012discovery} handles contacts in a clever way, using piecewise-constant, smooth contact variables and a continuation scheme.
    This allows complex motions to emerge from a quadratic optimization procedure.
    However, this method is very slow (2\% real time) and it generates only static motion (no jumping or slipping).
    Again, maybe we could use it as an expert.

\end{itemize}


\bibliographystyle{plainnat}
\bibliography{bib}

\end{document}

